{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1khq46I-mYIwg_EU-x6frkpJOx00gZHvX","authorship_tag":"ABX9TyMPyxfn3nSiw/q3e7s15quu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMvDyndPDxMV","executionInfo":{"status":"ok","timestamp":1683205440885,"user_tz":180,"elapsed":13140,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}},"outputId":"c766adff-e747-4774-b4e4-bc928f218937"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install gensim\n","!pip install xgboost\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vs5LwHTvAv2m","executionInfo":{"status":"ok","timestamp":1683205452460,"user_tz":180,"elapsed":11588,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}},"outputId":"0eb26834-2e9d-4b2d-ed19-4947205cf599"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (1.7.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.10.1)\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic\"\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qT1STOAMHojC","executionInfo":{"status":"ok","timestamp":1683205453619,"user_tz":180,"elapsed":1166,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}},"outputId":"7c8c78bc-6d26-4ae2-b74a-77605c5c6309"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic\n","/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic\n"]}]},{"cell_type":"code","source":["#baixe GloVe \n","#!wget http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s300.zip\n","#!unzip glove_s300.zip\n","#!wget http://143.107.183.175:22980/download.php?file=embeddings/glove/glove.6B.300d-pt.zip\n","#!unzip glove.6B.300d-pt.txt\n","\n","%cd \"/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic\"\n","!pwd\n","!unzip '/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic/glove_s300.zip'\n","\n","#glove_file = \"glove_s300.txt\"\n","#glove_embeddings = KeyedVectors.load_word2vec_format(glove_file, binary=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVz_wCT9Bb8U","outputId":"7fb00b0f-3216-41e0-fc4a-284d6d84e12a"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic\n","/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic\n","Archive:  /content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic/glove_s300.zip\n","replace glove_s300.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["import pandas as pd\n","from gensim.models import KeyedVectors\n","from sklearn.model_selection import train_test_split\n","\n","# Substitua o 'caminho_do_arquivo_treino.csv' pelo caminho real do seu arquivo CSV de treino\n","arquivo_csv_treino = '/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic/data/hate_speech_data_paula.csv'\n","dados_treino = pd.read_csv(arquivo_csv_treino)\n","\n","# Pré-processamento dos dados de treino\n","X = dados_treino.iloc[:, 0]  # Coluna dos dados preprocessados\n","y = dados_treino.iloc[:, 1]  # Coluna dos rótulos\n","\n","# Dividir os dados em conjuntos de treino e validação\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","train_df = pd.DataFrame({'text': X_train, 'labels': y_train})\n","eval_df = pd.DataFrame({'text': X_val, 'labels': y_val})"],"metadata":{"id":"TsF6XlVFesoT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import string\n","from gensim.parsing.preprocessing import remove_stopwords\n","from gensim.models import KeyedVectors\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Input\n","from tensorflow.keras.optimizers import Adam\n","import xgboost as xgb\n","\n","# Função de pré-processamento\n","def preprocess(text):\n","    text = text.lower()  # Transformar para minúsculas\n","    text = remove_stopwords(text)  # Remover stopwords usando Gensim\n","    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remover pontuação\n","    return text\n","\n","# Aplicar pré-processamento nos conjuntos de dados\n","train_df['text'] = train_df['text'].apply(preprocess)\n","eval_df['text'] = eval_df['text'].apply(preprocess)\n","\n","# Carregar word embeddings pré-treinados\n","#word_embeddings_file = '/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic/glove.6B.300d.txt'\n","#word_embeddings = KeyedVectors.load_word2vec_format(word_embeddings_file, binary=False)\n","\n","def convert_glove_to_word2vec(glove_file, output_file):\n","    with open(glove_file, 'r', encoding='utf-8') as f_in:\n","        lines = f_in.readlines()\n","        num_vectors = len(lines)\n","        vector_size = len(lines[0].split()) - 1\n","\n","    with open(output_file, 'w', encoding='utf-8') as f_out:\n","        f_out.write(f'{num_vectors} {vector_size}\\n')\n","        for line in lines:\n","            f_out.write(line)\n","\n","glove_file = '/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic/glove.6B.300d.txt'\n","word2vec_output_file = '/content/drive/MyDrive/Colab Notebooks/2023/SBBD_toxic/glove.6B.300d-word2vec.txt'\n","\n","convert_glove_to_word2vec(glove_file, word2vec_output_file)\n","word_embeddings = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n","\n","# Tokenização e padding\n","max_length = 280\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_df['text'])\n","word_index = tokenizer.word_index\n","\n","train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n","train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n","\n","eval_sequences = tokenizer.texts_to_sequences(eval_df['text'])\n","eval_padded = pad_sequences(eval_sequences, maxlen=max_length, padding='post', truncating='post')\n","\n","# Preparar matriz de embeddings\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_dim = 300\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","for word, i in word_index.items():\n","    if word in word_embeddings:\n","        embedding_vector = word_embeddings[word]\n","        embedding_matrix[i] = embedding_vector\n","\n","# Definir a arquitetura do modelo LSTM\n","inputs = Input(shape=(max_length,))\n","x = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)(inputs)\n","x = LSTM(50, return_sequences=True)(x)\n","x = Dropout(0.5)(x)\n","x = LSTM(50)(x)\n","x = Dropout(0.5)(x)\n","outputs = Dense(1, activation='sigmoid')(x)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","# Compilar e treinar o modelo\n","model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(train_padded, train_df['labels'], epochs=10, batch_size=128, validation_split=0.1)\n","\n","# Remover a última camada (softmax) do modelo para extrair características\n","feature_extractor = Model(inputs=model.input, outputs=model.layers[-2].output)\n","\n","#Extrair características do conjunto de teste\n","train_features = feature_extractor.predict(train_padded)\n","eval_features = feature_extractor.predict(eval_padded)\n","\n","#Treinar um classificador XGBoost com as características extraídas\n","xgb_classifier = xgb.XGBClassifier()\n","#xgb_classifier.fit(eval_features, eval_df['labels'])\n","xgb_classifier.fit(train_features, train_df['labels'])\n","\n","#Avaliar o classificador XGBoost\n","xgb_preds = xgb_classifier.predict(eval_features)\n","xgb_f1 = f1_score(eval_df['labels'], xgb_preds)\n","xgb_precision = precision_score(eval_df['labels'], xgb_preds)\n","xgb_recall = recall_score(eval_df['labels'], xgb_preds)\n","\n","print(f'XGBoost F1 Score: {xgb_f1}')\n","print(f'XGBoost Precision: {xgb_precision}')\n","print(f'XGBoost Recall: {xgb_recall}')\n"],"metadata":{"id":"IBAxmr1vCjtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_-v3wC4rKWJV"},"execution_count":null,"outputs":[]}]}